{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hdfs_fs = 'hdfs://hadoop-master.hadoop-domain.default-tenant.svc.cluster.local:9000'\n",
    "v3io_fs =  os.getenv('V3IO_HOME_URL')\n",
    "\n",
    "print(f\"HDFS: {hdfs_fs}\")\n",
    "print(f\"V3IO: {v3io_fs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krb5_cc_name = 'FILE:/User/spark/krb5kdc_ccache'\n",
    "hadoop_conf_dir = '/User/spark/hadoop/'\n",
    "krb5_config_file = '/User/spark/krb5.conf'\n",
    "krb5_keytab_file = '/User/spark/krb5.keytab'\n",
    "jvm_config_option = f\"-Dsun.zip.disableMemoryMapping=true -Djava.security.krb5.conf={krb5_config_file}\"\n",
    "\n",
    "# Use this to enable extra debug around Kerberos\n",
    "# jvm_config_option = jvm_config_option + \" -Dsun.security.krb5.debug=true\"\n",
    "\n",
    "print(f\"KRB5CCNAME: {krb5_cc_name}\")\n",
    "print(f\"HADOOP_CONF_DIR: {hadoop_conf_dir}\")\n",
    "print(f\"KRB5_CONFIG: {krb5_config_file}\")\n",
    "print(f\"JVM config: {jvm_config_option}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KRB5CCNAME'] = krb5_cc_name\n",
    "os.environ['HADOOP_CONF_DIR'] = hadoop_conf_dir\n",
    "os.environ['KRB5_CONFIG'] = krb5_config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kinit -k -t /User/spark/krb5.keytab hdfs/hadoop-master.hadoop-domain.default-tenant.svc.cluster.local@EXAMPLE.COM\n",
    "!klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Example\") \\\n",
    "    .master('k8s://https://kubernetes.default.svc:443') \\\n",
    "    .config('fs.v3io.impl','io.iguaz.v3io.hcfs.V3IOFileSystem') \\\n",
    "    .config('fs.AbstractFileSystem.v3io.impl','io.iguaz.v3io.hcfs.V3IOAbstractFileSystem') \\\n",
    "    .config('spark.kubernetes.container.image','spark-exec/spark-py:latest') \\\n",
    "    .config('spark.kubernetes.driver.pod.name', hostname) \\\n",
    "    .config('spark.kubernetes.namespace','default-tenant') \\\n",
    "    .config('spark.pyspark.python','python3.7') \\\n",
    "    .config('spark.kubernetes.executor.podTemplateFile','/User/spark/worker_pod.yaml') \\\n",
    "    .config('spark.executor.extraJavaOptions', jvm_config_option) \\\n",
    "    .config('spark.executorEnv.HADOOP_CONF_DIR', hadoop_conf_dir) \\\n",
    "    .config('spark.kerberos.keytab', krb5_keytab_file) \\\n",
    "    .config('spark.kerberos.principal','hdfs/hadoop-master.hadoop-domain.default-tenant.svc.cluster.local@EXAMPLE.COM') \\\n",
    "    .config('spark.kubernetes.kerberos.krb5.path', krb5_config_file) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3io_path = os.path.join(v3io_fs,'examples','demo.csv')\n",
    "print(v3io_path)\n",
    "\n",
    "v3io_df = spark.read.csv(v3io_path)\n",
    "v3io_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(hdfs_fs,'output.parquet')\n",
    "print(output_path)\n",
    "\n",
    "v3io_df.write.parquet(output_path, mode='overwrite')\n",
    "# v3io_df.write.csv(f'{hdfs_fs}/output.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_df = spark.read.parquet(output_path)\n",
    "hdfs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various optional configurations\n",
    "\n",
    "1. This will enable using the Hadoop native libs (rather than the libs packages with Spark). It will not work in k8s mode since the Pods created do not have the native Hadoop libraries installed (which can be changed if using a different Docker image for them).\n",
    "\n",
    "    `.config('spark.executorEnv.LD_LIBRARY_PATH', '/hadoop/lib/native') \\`\n",
    "\n",
    "2. Run in local mode (not using remote executors)\n",
    "\n",
    "    `.master('local') \\`\n",
    "\n",
    "3. These env variables are not needed in k8s mode.\n",
    "\n",
    "    `.config('spark.executorEnv.KRB5_CONFIG', krb5_config_file) \\`\n",
    "\n",
    "    `.config('spark.executorEnv.KRB5CCNAME', krb5_cc_name) \\`\n",
    "\n",
    "4. These seem to only work in Spark >=3.0\n",
    "\n",
    "    `.config('spark.kerberos.access.hadoopFileSystems', hdfs_fs) \\`\n",
    "\n",
    "    `.config('spark.kerberos.renewal.credentials','ccache') \\`\n",
    "\n",
    "5. Not needed in k8s mode. The driver seems to ignore the krb5.conf setting anyway, for some reason:\n",
    "\n",
    "    `.config('spark.driver.extraJavaOptions', jvm_config_option) \\`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup env variables, if needed for some reason.\n",
    "\n",
    "import os\n",
    "\n",
    "if os.environ.get('HADOOP_CONF_DIR'):\n",
    "    os.environ.pop('HADOOP_CONF_DIR')\n",
    "if os.environ.get('KRB5CCNAME'):\n",
    "   os.environ.pop('KRB5CCNAME')\n",
    "if os.environ.get('KRB5_CONFIG'):\n",
    "    os.environ.pop('KRB5_CONFIG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}