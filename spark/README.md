# Spark in k8s mode supporting secure HDFS storage

It seems that running Spark stand-alone against secure HDFS does not work. The executors have no facility to authenticate with Kerberos, and hence cannot access files on a secure HDFS cluster. However, it is possible to execute the same with Spark in k8s mode. This was tested against Spark 3.0, as 3.0 has some additional features that are not present in 2.4.X, and are necessary for this functionality to work.
To make this work, you need to execute the following steps.

## Pre-requisite: Deploy Jupyter with Spark support

Through Iguazio UI, enable a Spark service, and then create a Jupyter service that uses the same. Once this is done, you have the Jupyter service with Spark pre-deployed in it.
The pod running the Spark driver (which is the Jupyter pod) will need to have permissions to perform (see <https://spark.apache.org/docs/latest/running-on-kubernetes.html#prerequisites>) list, create, edit and delete pods in the cluster. OOTB, the Jupyter service has all of them except for edit. This means that you need to:

1. Add pod edit permissions to the `jupyter-basic-job-executor` role
    > Note: while the documentation calls for this, it looks like everything works even without it. However, it's recommended that we add it, just to make sure.

## Create Spark Docker images

The k8s mode for Spark deployed the executors in pods that Spark generates based on an image that the caller provides.
Spark does provide a facility to generate the correct Docker images, which include the various packages needed. It should be noted that to run Python code, a different image is needed (which also can be generated by the same script).
Jupyter does not have Docker installed on it, so for now I performed the following on the k8s node itself:

1. Retrieve the Spark deployment from the Jupyter pod - basically `tar` everything under `/spark` and copy it to the node.
   > It's critical that it's the same Spark distribution and even minor version. I started with downloading Spark 3.0.1 while Jupyter runs 3.0.0, and the executors could not communicate with the driver.

2. Under the directory where you downloaded the Spark distribution, perform the following command:

   ```bash
   ./bin/docker-image-tool.sh -r spark-exec -t latest -u 1000 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
   ```
  
   This generates 2 images: `spark-exec/spark` and `spark-exec/spark-py`. The `-r` and `-t` parameters tell the command how to label the images. The `-p` parameter tells it to create the Python image as well as the basic image. The `-u` command tells it to set `USER` for the docker image to be 1000, which is the same user other containers use for `iguazio` - this turned out to be critical, otherwise the `v3iod` communication failed due to files in `shm` not created due to wrong permissions.

The command has other parameters which for example allow you to perform various modifications to the image, and also to push the images to a repository - in our case it's not necessary since we're using the local Docker repository.

## Prepare Jupyter to execute code and copy files

For Spark to work properly against a secure HDFS we need the following files available on Jupyter and on the executors:

1. Kerberos configurations - `krb5.conf`, `krb5.keytab`
2. HDFS configurations - `core-site.xml` and `hdfs-site.xml`
3. Executor pod customization template - the executors will need access to `/User` which means that they will need fuse mounted to them. This is no different than any other pod that needs file-system access via fuse. To enable this we need to mount volumes to the pods, and while Spark has some support for that, it does not support `FlexVolume` mounts. Therefore, we need to use a template that adds these configurations to executor pods. For that we have this [yaml file](./worker_pod.yaml) which needs to be accessible to the Jupyter driver

Besides those files, there are also a [notebook file](./spark-k8s.ipynb) that has the Spark code needed to perform HDFS access, and a nice [script](create_jupyter_env) that creates a `/User/spark` directory on fuse, and populates it with the needed files. Some of these files are retrieved from the `hadoop-worker` pod, so make sure you have it running.
Once you have configured everything, you can go ahead and run the notebook from Jupyter, and as always - pray. Pray a lot.

> ## BUG: Having to place `krb5.conf` in `/etc`
>
> One huge caveat we have so far is that on the Jupyter pod, the driver for some reason requires that the `krb5.conf` file be present in the default location on `/etc`. I found no way to work around that for now. This is still WIP.

## Actual Spark configurations

The [notebook](./spark-k8s.ipynb) contains several sections with critical configurations, without which Spark will not be able to work properly. This section explains the specific parts of it and their role in the overall execution.

### Local environment variables

The Spark driver is running on the Jupyter pod in our case, since this is client mode, which is the only mode supported for Python in Spark. The driver has responsiblity for communicating with the HDFS cluster and obtaining delegation tokens to be passed to the executors. This means that the driver needs to be able to authenticate to HDFS using Kerberos. To do that, it needs the "usual suspects" of environment variables:

1. `KRB5CCNAME` - the location of the Kerberos ticket cache file
2. `HADOOP_CONF_DIR` - HDFS configuration. Without this it won't understand it needs to work securely, and who to talk to
3. `KRB5_CONFIG` - location of the Kerberos configuration. Note that per the BUG mentioned above, this has no effect for some reason

The notebook also contains code to run `kinit` to make sure it's authenticated with Kerberos. In the final product the plan is for this to be replaced with a sidecar implementation or something else.

### Spark session configurations

This section will explain what's in this big clause:

```bash
spark = SparkSession.builder.appName("Example") \
    .master('k8s://https://kubernetes.default.svc:443') \
    .config('fs.v3io.impl','io.iguaz.v3io.hcfs.V3IOFileSystem') \
    .config('fs.AbstractFileSystem.v3io.impl','io.iguaz.v3io.hcfs.V3IOAbstractFileSystem') \
    .config('spark.kubernetes.container.image','spark-exec/spark-py:latest') \
    .config('spark.kubernetes.driver.pod.name', hostname) \
    .config('spark.kubernetes.namespace','default-tenant') \
    .config('spark.pyspark.python','python3.7') \
    .config('spark.kubernetes.executor.podTemplateFile','/User/spark/worker_pod.yaml') \
    .config('spark.executor.extraJavaOptions', jvm_config_option) \
    .config('spark.executorEnv.HADOOP_CONF_DIR', hadoop_conf_dir) \
    .config('spark.kerberos.keytab', krb5_keytab_file) \
    .config('spark.kerberos.principal','hdfs/hadoop-master.hadoop-domain.default-tenant.svc.cluster.local@EXAMPLE.COM') \
    .config('spark.kubernetes.kerberos.krb5.path', krb5_config_file) \
    .getOrCreate()
```

It contains the following configuation parameters:

1. `.master('k8s://https://kubernetes.default.svc:443')` - this tells Spark that we work in k8s mode, and what is the address of the `k8s` API service. Note that both the `k8s://` and `https://` clauses are needed - if only the `k8s` part is kept, Spark will attempt `http` and fail in our case
2. `'fs.v3io.impl'`, `'fs.AbstractFileSystem.v3io.impl'` - v3io is actually overriding HDFS configurations to enable accessing files through the `v3io://` prefix. When using HDFS, some stepping-on-toes happens and Spark doesn't have these configurations available through config files. Therefore, they need to be provided through configurations
3. `'spark.kubernetes.container.image'`, `'spark.kubernetes.namespace'` - configurations that tell Spark what Docker image to use, and what namespace to use. The default namespace is `default`, so that won't work in our case. Of course, one can automatically take the value from `/var/run/secrets/kubernetes.io/serviceaccount/namespace` or some env. variable. Currently I've used a manually-provided value
4. `'spark.pyspark.python'` - the Python image has both Python 2 and Python 3 implementations on it (possibly this can be overriden in the image creation script). By default it tries to run the code using Python 2, so this parameter is needed to make it use the right one.
5. `'spark.kubernetes.executor.podTemplateFile'` - this is the Pod template file, described above
6. `'spark.kubernetes.driver.pod.name'` - this parameter is not mandatory, but it makes our pod the owner of the executor pods, so that if the Jupyter pod is deleted all executor pods will also get deleted by k8s
7. `'spark.executor.extraJavaOptions'` - I use this to pass the `java.security.krb5.conf` parameter to the executor JVM. This is also passed through the `'spark.kubernetes.kerberos.krb5.path'` parameter, so one of them seems to be redundant. Will need to check which one...
8. `'spark.executorEnv.HADOOP_CONF_DIR'` - tells the executor where to find the Hadoop configuration. Critical parameter
9. `'spark.kerberos.keytab'` and `'spark.kerberos.principal'` - these of course are critical so that Spark knows how to authenticate to Kerberos, and enable renewing of the Kerberos tickets once they expire (for long running tasks)

The notebook also has a small section on some other confgurations which exist and may or not bring any benefit at all.

## Deployment steps for customer environment

**Pre-requisite:** Spark service is deployed, running Spark >=3.0.0.

### Deploy Jupyter

when deploying Jupyter service, make sure to:

1. Add Spark support as part of the deployment, pick the Spark service. We are not going to use this spark service, but this option adds configurations to Jupyter that we'll need for k8s mode as well. Besides, if the user wants to work with the stand-alone Spark service, he can do it.
2. Add environment variables. The following environment vars should be added:

    ```bash
    KRB5CCNAME = FILE:/User/conf/kerberos/krb5_ccache
    HADOOP_CONF_DIR = /User/conf/hadoop
    KRB5_CONFIG = /User/conf/kerberos/krb5.conf
    ```

    Make sure to write them exactly as they're shown here. No need for any modifications in them.

### Create /User file heirarchy

Created filesystem heirarchy with the configuration files:

```bash
User/conf/hadoop:
    core-site.xml
    hdfs-site.xml

User/conf/kerberos:
    krb5.conf
    krb5.keytab
    krb5_ccache

User/conf/spark:
    worker_pod.yaml
```

It's ok to not have those files available at first delpoyment, as we need the customer to provide them (they are his Kerberos and Hadoop configurations), but the directory structure should be there, and the customer should be instructed to place the files in the correct locations, and use the exact names.

> **Note:** I modified the `krb5.conf` file so that the ticket cache location is changed to the right location (`/User/conf/kerberos/krb5_ccache`). I did this with the command:
>
> ```sed -i 's/\/tmp\/ccache\/krb5kdc_ccache/\/User\/conf\/kerberos\/krb5kdc_ccache/g' ./conf/kerberos/krb5.conf```
>
> A similar thing will need to be done for the customer's config file as well. Alternatively, if we're setting the `KRB5CCNAME` env variable, then this can be avoided as it overrides the configuration.

### Place `krb5.conf` in `/etc`

>**Note:** The `krb5.conf` file is a configuration file that should be <
provided by the customer. If we can have it available at deployment, then this step can be done then. Else, need to escort the customer on first usage.

Copy `krb5.conf` also to `/etc/krb5.conf` on Jupyter (currently it's the only location that it will accept). Unfortunately, this needs to be done as root, so from the node ssh do this:

```bash
JUPYTER_CONTAINER_ID=`docker ps|grep jupyter|grep bash|awk '{print $1}'`
docker exec -it -u root $JUPYTER_CONTAINER_ID cp /User/conf/kerberos/krb5.conf /etc/krb5.conf
```

### Create Spark executor image

Need to have the Spark distribution we are using in Jupyter, and from it generate the spark Docker images, using the command:

```bash
cd <spark root dir>
./bin/docker-image-tool.sh -r spark-exec -t latest -u 1000 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
```

Ensure both images (basic and python) were created:

```bash
$ docker images | grep spark-exec
spark-exec/spark-py                                                                                                               latest                                  f059bac69989        13 seconds ago       992MB
spark-exec/spark                                                                                                                  latest                                  22f1da7cc733        About a minute ago   522MB
```

### Pod template yaml file

Use this [yaml file](./worker_pod.yaml).

v3io_auth is reusing a k8s secret that Jupyter uses, so you need to ensure that the `jupyter-v3io-auth` secret exists (and if your Jupyter service and hence the secret is named differently, then you need to modify the template accordingly):

```yaml
  volumes:
  - name: v3io-auth
    secret:
      defaultMode: 420
      secretName: jupyter-v3io-auth
```

Also, it sets `IGZ_DATA_CONFIG_FILE` to point at `/User/conf/spark/v3io.conf`, so we need to place the file there. On shell service or Jupyter service, perform:

```bash
cp $IGZ_DATA_CONFIG_FILE /User/conf/spark/
```

### Modify `spark-defaults.conf` file

Start with the existing file (in `/spark/conf/spark-defaults.conf`) - it should be there if the Jupyter service was created with Spark support.

Add the following lines to the file:

```bash
# Configurations that are specific to HDFS/Kerberos with Spark k8s mode.

spark.hadoop.fs.v3io.impl=io.iguaz.v3io.hcfs.V3IOFileSystem
spark.hadoop.fs.AbstractFileSystem.v3io.impl=io.iguaz.v3io.hcfs.V3IOAbstractFileSystem
spark.kubernetes.container.image=spark-exec/spark-py:latest
spark.kubernetes.namespace=default-tenant
spark.pyspark.python=python3.7
spark.kubernetes.executor.podTemplateFile=/User/conf/spark/worker_pod.yaml
spark.executorEnv.HADOOP_CONF_DIR=/User/conf/hadoop
spark.kerberos.keytab=/User/conf/kerberos/krb5.keytab
```

As there's nothing specific in these lines, you can copy them as-is. No need for any modifications.

### Verify Kerberos init works

From Jupyter shell, execute the following lines:

```bash
kinit -k -t /User/conf/kerberos/krb5.keytab <user-principal>@<realm>
klist
```

If it works, then we can indeed authenticate with Kerberos using the provided keytab and the needed principal.
The same lines can be executed from a notebook, just prefix them with `!`.

### Spark configuration needed by the user

As most of the parameters are pre-configured at this stage, the user needs to do very little configuration in the actual Python notebook. Specifically the line to create a `SparkSession` should be something like:

```python
spark = SparkSession.builder.appName('<app name>') \
    .master('k8s://https://kubernetes.default.svc:443') \
    .config('spark.kerberos.principal','<principal name>') \
    .getOrCreate()
```

See the [notebook](./spark-k8s.ipynb) for a working example.

## Open issues / questions

Question: Jupyter refuses to import `pyspark.sql` - brings out error:

```python
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-6-384785b29356> in <module>
----> 1 from pyspark.sql import SparkSession
      2 import socket
      3 
      4 hostname = socket.gethostname()
      5 

/spark/python/pyspark/__init__.py in <module>
     49 
     50 from pyspark.conf import SparkConf
---> 51 from pyspark.context import SparkContext
     52 from pyspark.rdd import RDD, RDDBarrier
     53 from pyspark.files import SparkFiles

/spark/python/pyspark/context.py in <module>
     25 from tempfile import NamedTemporaryFile
     26 
---> 27 from py4j.protocol import Py4JError
     28 from py4j.java_gateway import is_instance_of
     29 

ModuleNotFoundError: No module named 'py4j'
```

This was solved by:

```!pip install py4j==0.10.9```

Maybe we want to pre-install it on Jupyter? 
