# Spark in k8s mode supporting secure HDFS storage

It seems that running Spark stand-alone against secure HDFS does not work. The executors have no facility to authenticate with Kerberos, and hence cannot access files on a secure HDFS cluster. However, it is possible to execute the same with Spark in k8s mode. This was tested against Spark 3.0, as 3.0 has some additional features that are not present in 2.4.X, and are necessary for this functionality to work.
To make this work, you need to execute the following steps.

## Pre-requisite: Deploy Jupyter with Spark support

Through Iguazio UI, enable a Spark service, and then create a Jupyter service that uses the same. Once this is done, you have the Jupyter service with Spark pre-deployed in it.
The pod running the Spark driver (which is the Jupyter pod) will need to have permissions to perform (see <https://spark.apache.org/docs/latest/running-on-kubernetes.html#prerequisites>) list, create, edit and delete pods in the cluster. OOTB, the Jupyter service has all of them except for edit. This means that you need to:

1. Add pod edit permissions to the `jupyter-basic-job-executor` role
    > Note: while the documentation calls for this, it looks like everything works even without it. However, it's recommended that we add it, just to make sure.

## Create Spark Docker images

The k8s mode for Spark deployed the executors in pods that Spark generates based on an image that the caller provides.
Spark does provide a facility to generate the correct Docker images, which include the various packages needed. It should be noted that to run Python code, a different image is needed (which also can be generated by the same script).
Jupyter does not have Docker installed on it, so for now I performed the following on the k8s node itself:

1. Retrieve the Spark deployment from the Jupyter pod - basically `tar` everything under `/spark` and copy it to the node.
   > It's critical that it's the same Spark distribution and even minor version. I started with downloading Spark 3.0.1 while Jupyter runs 3.0.0, and the executors could not communicate with the driver.

2. Under the directory where you downloaded the Spark distribution, perform the following command:

   ```bash
   ./bin/docker-image-tool.sh -r spark-exec -t latest -u 1000 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
   ```
  
   This generates 2 images: `spark-exec/spark` and `spark-exec/spark-py`. The `-r` and `-t` parameters tell the command how to label the images. The `-p` parameter tells it to create the Python image as well as the basic image. The `-u` command tells it to set `USER` for the docker image to be 1000, which is the same user other containers use for `iguazio` - this turned out to be critical, otherwise the `v3iod` communication failed due to files in `shm` not created due to wrong permissions.

The command has other parameters which for example allow you to perform various modifications to the image, and also to push the images to a repository - in our case it's not necessary since we're using the local Docker repository.

## Prepare Jupyter to execute code and copy files

For Spark to work properly against a secure HDFS we need the following files available on Jupyter and on the executors:

1. Kerberos configurations - `krb5.conf`, `krb5.keytab`
2. HDFS configurations - `core-site.xml` and `hdfs-site.xml`
3. Executor pod customization template - the executors will need access to `/User` which means that they will need fuse mounted to them. This is no different than any other pod that needs file-system access via fuse. To enable this we need to mount volumes to the pods, and while Spark has some support for that, it does not support `FlexVolume` mounts. Therefore, we need to use a template that adds these configurations to executor pods. For that we have this [yaml file](./worker_pod.yaml) which needs to be accessible to the Jupyter driver

Besides those files, there are also a [notebook file](./spark-k8s.ipynb) that has the Spark code needed to perform HDFS access, and a nice [script](create_jupyter_env) that creates a `/User/spark` directory on fuse, and populates it with the needed files. Some of these files are retrieved from the `hadoop-worker` pod, so make sure you have it running.
Once you have configured everything, you can go ahead and run the notebook from Jupyter, and as always - pray. Pray a lot.

> ## BUG: Having to place `krb5.conf` in `/etc`
>
> One huge caveat we have so far is that on the Jupyter pod, the driver for some reason requires that the `krb5.conf` file be present in the default location on `/etc`. I found no way to work around that for now. This is still WIP.

## Actual Spark configurations

The [notebook](./spark-k8s.ipynb) contains several sections with critical configurations, without which Spark will not be able to work properly. This section explains the specific parts of it and their role in the overall execution.

### Local environment variables

The Spark driver is running on the Jupyter pod in our case, since this is client mode, which is the only mode supported for Python in Spark. The driver has responsiblity for communicating with the HDFS cluster and obtaining delegation tokens to be passed to the executors. This means that the driver needs to be able to authenticate to HDFS using Kerberos. To do that, it needs the "usual suspects" of environment variables:

1. `KRB5CCNAME` - the location of the Kerberos ticket cache file
2. `HADOOP_CONF_DIR` - HDFS configuration. Without this it won't understand it needs to work securely, and who to talk to
3. `KRB5_CONFIG` - location of the Kerberos configuration. Note that per the BUG mentioned above, this has no effect for some reason

The notebook also contains code to run `kinit` to make sure it's authenticated with Kerberos. In the final product the plan is for this to be replaced with a sidecar implementation or something else.

### Spark session configurations

This section will explain what's in this big clause:

```bash
spark = SparkSession.builder.appName("Example") \
    .master('k8s://https://kubernetes.default.svc:443') \
    .config('fs.v3io.impl','io.iguaz.v3io.hcfs.V3IOFileSystem') \
    .config('fs.AbstractFileSystem.v3io.impl','io.iguaz.v3io.hcfs.V3IOAbstractFileSystem') \
    .config('spark.kubernetes.container.image','spark-exec/spark-py:latest') \
    .config('spark.kubernetes.driver.pod.name', hostname) \
    .config('spark.kubernetes.namespace','default-tenant') \
    .config('spark.pyspark.python','python3.7') \
    .config('spark.kubernetes.executor.podTemplateFile','/User/spark/worker_pod.yaml') \
    .config('spark.executor.extraJavaOptions', jvm_config_option) \
    .config('spark.executorEnv.HADOOP_CONF_DIR', hadoop_conf_dir) \
    .config('spark.kerberos.keytab', krb5_keytab_file) \
    .config('spark.kerberos.principal','hdfs/hadoop-master.hadoop-domain.default-tenant.svc.cluster.local@EXAMPLE.COM') \
    .config('spark.kubernetes.kerberos.krb5.path', krb5_config_file) \
    .getOrCreate()
```

It contains the following configuation parameters:

1. `.master('k8s://https://kubernetes.default.svc:443')` - this tells Spark that we work in k8s mode, and what is the address of the `k8s` API service. Note that both the `k8s://` and `https://` clauses are needed - if only the `k8s` part is kept, Spark will attempt `http` and fail in our case
2. `'fs.v3io.impl'`, `'fs.AbstractFileSystem.v3io.impl'` - v3io is actually overriding HDFS configurations to enable accessing files through the `v3io://` prefix. When using HDFS, some stepping-on-toes happens and Spark doesn't have these configurations available through config files. Therefore, they need to be provided through configurations
3. `'spark.kubernetes.container.image'`, `'spark.kubernetes.namespace'` - configurations that tell Spark what Docker image to use, and what namespace to use. The default namespace is `default`, so that won't work in our case. Of course, one can automatically take the value from `/var/run/secrets/kubernetes.io/serviceaccount/namespace` or some env. variable. Currently I've used a manually-provided value
4. `'spark.pyspark.python'` - the Python image has both Python 2 and Python 3 implementations on it (possibly this can be overriden in the image creation script). By default it tries to run the code using Python 2, so this parameter is needed to make it use the right one.
5. `'spark.kubernetes.executor.podTemplateFile'` - this is the Pod template file, described above
6. `'spark.kubernetes.driver.pod.name'` - this parameter is not mandatory, but it makes our pod the owner of the executor pods, so that if the Jupyter pod is deleted all executor pods will also get deleted by k8s
7. `'spark.executor.extraJavaOptions'` - I use this to pass the `java.security.krb5.conf` parameter to the executor JVM. This is also passed through the `'spark.kubernetes.kerberos.krb5.path'` parameter, so one of them seems to be redundant. Will need to check which one...
8. `'spark.executorEnv.HADOOP_CONF_DIR'` - tells the executor where to find the Hadoop configuration. Critical parameter
9. `'spark.kerberos.keytab'` and `'spark.kerberos.principal'` - these of course are critical so that Spark knows how to authenticate to Kerberos, and enable renewing of the Kerberos tickets once they expire (for long running tasks)

The notebook also has a small section on some other confgurations which exist and may or not bring any benefit at all.

## Steps for productization

This section describes the steps needed to enable the Spark k8s mode to be working within the platform in a reasonable manner. This doesn't mean full support by the platform, but rather a manually-configured but easily activated mode that can be enabled if requested.

### Creating a container for executor pods

An image needs to exist that supports running Spark with the correct version, and supports executing Python code on top. This image needs to be pointed to by the `spark.kubernetes.container.image` parameter of Spark configuration.

> **TBD**: Can we use the Spark server image for this purpose? Re-using the Spark server image has benefits in that it does not require creating a new image, and it is guaranteed to be fully compatible with the client version of Spark running on Jupyter (for example).

### Generating a pod template file

As described above, we need to generate a pod tamplate that will serve as basis for the executor pods. This template needs to have the `/User` fuse mount, and the facilities needed for the `v3iod` deamon to operate on top of it - these require some shared-memory mappings, for example.

If we're using the Spark server image as basis for our pods, this step may be less required, since the image already contains within it most of the configurations needed - this may be another benefit for using the Spark server image.

Should we choose to create a pod template file, it will need to be placed in the pods that execute the Spark client code, so that the Spark submit operation can reach it.

### Configurations not done by the user

#### Spark default configurations

As described in the previous sections, there are various configurations that are needed in order for Spark k8s mode to work properly with HDFS and Kerberos. We'd like to make the customer customize as few of these as possible, and for that we can providede a `spark.defaults` file that will already contain most of the configurations needed. Since there's only one defaults file for Spark, if the uses wishes to work both with stand-alone mode and with k8s mode, then we'll need to provide a capability to switch between configuration files - we can generate a script that will do that.

The following parameters should be in the default configuration file (and not need configuring by the user):

1. `.master('k8s://https://kubernetes.default.svc:443')`
2. `fs.v3io.impl`, `fs.AbstractFileSystem.v3io.impl`
3. `spark.kubernetes.container.image`, `spark.kubernetes.namespace`
4. `spark.pyspark.python`
5. `spark.kubernetes.executor.podTemplateFile` - if we need a pod template
6. `spark.kubernetes.driver.pod.name` - can be set to the pod's name (or `hostname` in most cases)
7. `spark.executor.extraJavaOptions`

Except for the 1st parameter (which implies k8s mode), the rest of the parameters can be set also for stand-alone mode, as they either have no meaning in stand-alone mode or do not interfere with the regular operation of it (of course, this needs to be verified).

What this means is that the only configuration that is reqlly unique is setting the `master` property for Spark - this can actually be done as part of the Python code if we want to allow the user to choose between stand-alone and k8s. In this case we use the exact same defaults file, and the user just modifies the master per the mode she wishes to work with.

#### Environment variables

1. `KRB5CCNAME` - the location of the Kerberos ticket cache file
2. `HADOOP_CONF_DIR` - HDFS configuration. Without this it won't understand it needs to work securely, and who to talk to
3. `KRB5_CONFIG` - location of the Kerberos configuration. Note that per the BUG mentioned above, this has no effect for some reason

We can decide that we pre-set those parameters, and require that the user conform to the values we specify. For example, decide that the Hadoop configuration will be always placed in `/User/conf/hadoop` and the relevant Kerberos files will be always placed in `/User/conf/kerberos`. We can then point these env variables to the relevant locations. For example:

```bash
export HADOOP_CONF_DIR=/User/conf/hadoop
export KRB5_CONFIG=/User/conf/kerberos/krb5.conf
export KRB5CCNAME=/User/conf/kerberos/krb5_ccache
```

The only thing here that may be delicate is that the ticket cache file is not always with the same default name, and the user may wish to modify it (though it is determined in the `krb5.conf` file). We can still enforce this and work with the user to make sure the configuration matches.

### User configurations

With all the pre-configuration mentioned above, we are actually left with a very limited set of configurations that the user needs to configure.
There are 3 configurations that are passed to Spark and point at HDFS/Kerberos configurations:

1. `spark.executorEnv.HADOOP_CONF_DIR`
2. `spark.kerberos.keytab` and `spark.kerberos.principal`

If we pre-determine the `HADOOP_CONF_DIR` value, then this can also be pre-determined and passed to the default config file. The other two parameters will still need to be specified by the user, as they cannot be pre-determined by us.

The bottom line is, the user will need to specify 3 parameters:

1. `.master('k8s://https://kubernetes.default.svc:443')`
2. `spark.kerberos.keytab`
3. `spark.kerberos.principal`

Which is a very easy to do configuration.