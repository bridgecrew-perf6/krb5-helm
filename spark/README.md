# Spark in k8s mode supporting secure HDFS storage

It seems that running Spark stand-alone against secure HDFS does not work. The executors have no facility to authenticate with Kerberos, and hence cannot access files on a secure HDFS cluster. However, it is possible to execute the same with Spark in k8s mode. This was tested against Spark 3.0, as 3.0 has some additional features that are not present in 2.4.X, and are necessary for this functionality to work.
To make this work, you need to execute the following steps.

## Pre-requisite: Deploy Jupyter with Spark support

Through Iguazio UI, enable a Spark service, and then create a Jupyter service that uses the same. Once this is done, you have the Jupyter service with Spark pre-deployed in it.
The pod running the Spark driver (which is the Jupyter pod) will need to have permissions to perform (see <https://spark.apache.org/docs/latest/running-on-kubernetes.html#prerequisites>) list, create, edit and delete pods in the cluster. OOTB, the Jupyter service has all of them except for edit. This means that you need to:

1. Add pod edit permissions to the `jupyter-basic-job-executor` role
    > Note: while the documentation calls for this, it looks like everything works even without it. However, it's recommended that we add it, just to make sure.

## Create Spark Docker images

The k8s mode for Spark deployed the executors in pods that Spark generates based on an image that the caller provides.
Spark does provide a facility to generate the correct Docker images, which include the various packages needed. It should be noted that to run Python code, a different image is needed (which also can be generated by the same script).
Jupyter does not have Docker installed on it, so for now I performed the following on the k8s node itself:

1. Retrieve the Spark deployment from the Jupyter pod - basically `tar` everything under `/spark` and copy it to the node.
   > It's critical that it's the same Spark distribution and even minor version. I started with downloading Spark 3.0.1 while Jupyter runs 3.0.0, and the executors could not communicate with the driver.

2. Under the directory where you downloaded the Spark distribution, perform the following command:

   ```bash
   ./bin/docker-image-tool.sh -r spark-exec -t latest -u 1000 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
   ```
  
   This generates 2 images: `spark-exec/spark` and `spark-exec/spark-py`. The `-r` and `-t` parameters tell the command how to label the images. The `-p` parameter tells it to create the Python image as well as the basic image. The `-u` command tells it to set `USER` for the docker image to be 1000, which is the same user other containers use for `iguazio` - this turned out to be critical, otherwise the `v3iod` communication failed due to files in `shm` not created due to wrong permissions.

The command has other parameters which for example allow you to perform various modifications to the image, and also to push the images to a repository - in our case it's not necessary since we're using the local Docker repository.

## Prepare Jupyter to execute code and copy files

For Spark to work properly against a secure HDFS we need the following files available on Jupyter and on the executors:

1. Kerberos configurations - `krb5.conf`, `krb5.keytab`
2. HDFS configurations - `core-site.xml` and `hdfs-site.xml`
3. Executor pod customization template - the executors will need access to `/User` which means that they will need fuse mounted to them. This is no different than any other pod that needs file-system access via fuse. To enable this we need to mount volumes to the pods, and while Spark has some support for that, it does not support `FlexVolume` mounts. Therefore, we need to use a template that adds these configurations to executor pods. For that we have this [yaml file](./worker_pod.yaml) which needs to be accessible to the Jupyter driver

Besides those files, there are also a [notebook file](./spark-k8s.ipynb) that has the Spark code needed to perform HDFS access, and a nice [script](set_spark_env) that creates a `/User/spark` directory on fuse, and populates it with the needed files. Some of these files are retrieved from the `hadoop-worker` pod, so make sure you have it running.
Once you have configured everything, you can go ahead and run the notebook from Jupyter, and as always - pray. Pray a lot.

> ## BUG: Having to place `krb5.conf` in `/etc`
>
> One huge caveat we have so far is that on the Jupyter pod, the driver for some reason requires that the `krb5.conf` file be present in the default location on `/etc`. I found no way to work around that for now. This is still WIP.
